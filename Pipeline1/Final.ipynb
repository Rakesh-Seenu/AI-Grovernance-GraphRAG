{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing AI-Governance with GraphRAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Webscraping \n",
    "2. Setup ChromaDB\n",
    "3. Pre-processing \n",
    "4. Set up GraphRAG\n",
    "5. Retrieval-Augmented Generation\n",
    "6. Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Workflow of the model](Workflow_.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imporitng Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsrak\\Desktop\\case_study\\AI_governance\\mynewenv1\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hsrak\\Desktop\\case_study\\AI_governance\\mynewenv1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "import networkx as nx  # For GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `import os`: \n",
    "    - Provides functions to interact with the operating system, such as file and directory manipulation.\n",
    "\n",
    "- `import requests`:\n",
    "    - A library for sending HTTP requests in Python, used to fetch content from the web.\n",
    "    \n",
    "- `from bs4 import BeautifulSoup`:\n",
    "    - Imports the BeautifulSoup class from the bs4 library, which is used for parsing and navigating HTML and XML documents.\n",
    "\n",
    "- `import PyPDF2`:\n",
    "    - A Python library used to work with PDF files, allowing for reading and manipulation of PDFs.\n",
    "\n",
    "- `import chromadb`:\n",
    "    - A library for interacting with Chroma, an open-source embedding database used for storing and retrieving high-dimensional vector embeddings.\n",
    "\n",
    "- `from langchain_community.embeddings import OllamaEmbeddings`:\n",
    "    - Imports the OllamaEmbeddings class from the langchain_community.embeddings module, which facilitates generating embeddings using Ollama models.\n",
    "    \n",
    "- `from langchain_text_splitters import RecursiveCharacterTextSplitter`:\n",
    "    - Imports the RecursiveCharacterTextSplitter class from langchain_text_splitters, used for dividing large text documents into smaller segments based on character count, useful for processing large texts.\n",
    "\n",
    "- `from langchain_community.vectorstores import Chroma`:\n",
    "    - Imports the Chroma class from langchain_community.vectorstores, allowing interaction with the Chroma vector database for efficient vector storage and retrieval.\n",
    "\n",
    "- `from langchain.docstore.document import Document`:\n",
    "    - Imports the Document class from langchain.docstore.document, used to represent and handle documents for storage and retrieval tasks.\n",
    "    \n",
    "- `from langchain.prompts import ChatPromptTemplate, PromptTemplate`:\n",
    "    - Imports ChatPromptTemplate and PromptTemplate classes for generating customizable prompts for chat-based language models.\n",
    "\n",
    "- `from langchain_core.output_parsers import StrOutputParser`:\n",
    "    - Imports the StrOutputParser from langchain_core.output_parsers, which converts the output of language models into strings for easier handling and interpretation.\n",
    "    \n",
    "- `from langchain_community.chat_models import ChatOllama`:\n",
    "    - Imports the ChatOllama model, allowing interaction with the Ollama chat model for conversational AI tasks.\n",
    "\n",
    "- `from langchain_core.runnables import RunnablePassthrough`:\n",
    "    - Imports the RunnablePassthrough class from langchain_core.runnables, which passes inputs directly to the output without any modifications or processing.\n",
    "\n",
    "- `from langchain.retrievers.multi_query import MultiQueryRetriever`:\n",
    "    - Imports the MultiQueryRetriever class for performing multi-query retrieval from a document store, allowing for efficient and flexible search capabilities.\n",
    "\n",
    "- `from sentence_transformers import SentenceTransformer`:\n",
    "    - Imports SentenceTransformer, a library for generating dense vector representations (embeddings) from sentences using pre-trained transformer models.\n",
    "\n",
    "- `import uuid`:\n",
    "    - A library for generating unique identifiers (UUIDs), useful for creating unique keys or IDs for documents and processes.\n",
    "\n",
    "- `import networkx as nx`:\n",
    "    - Imports the networkx library, which is used for creating and manipulating complex networks (graphs), and is useful in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB client\n",
    "- ` What is ChromaDB` : \n",
    "    - ChromaDB is a vector database designed for handling large-scale AI and machine learning workloads. In a vector database, data is stored as mathematical vectors (or embeddings), which represent complex, high-dimensional information such as images, text, or other unstructured data. This type of database allows efficient similarity searches, where similar pieces of data are grouped or retrieved based on their proximity in vector space.\n",
    "\n",
    "- `Working`:\n",
    "    - pythonCopy codeclient = chromadb.Client()\n",
    "    - chromadb: This is the Python library or package for ChromaDB that provides an API to interact with the vector database.\n",
    "    - Client(): This is the class that initiates a connection to ChromaDB. The Client object serves as the main entry point for interacting with the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping\n",
    "\n",
    "- **`What is BeautifulSoup`:** \n",
    "     - BeautifulSoup is a Python library used to parse HTML and XML documents. It provides tools to navigate the HTML structure, extract specific elements, and modify or clean up the data. It is commonly used alongside requests (which fetches the HTML content from a webpage).\n",
    "\n",
    "- **`Working`:**\n",
    "    - 1. HTTP Requests: The requests library is used to send an HTTP GET request to the webpage, fetching the raw HTML.\n",
    "    - 2. HTML Parsing: BeautifulSoup parses the fetched HTML into a structured format (like a tree) that makes it easier to search for specific HTML elements (e.g., headings, paragraphs).\n",
    "    - 3. Navigating the HTML Tree: The code searches for specific tags and extracts their content based on class names or HTML structure.\n",
    "    \n",
    "\n",
    "Here we are using BeautifulSoup to scrape AI regulatory information from dynamically constructed URLs for one country. It sends an HTTP GET request via the requests library to retrieve webpage content. If successful (status code 200), the HTML is parsed to find specific elements containing article content.\n",
    "\n",
    "Text from headings and paragraphs is extracted in a loop. Headings create new sections in the country data, while paragraphs are added under the latest heading. If no heading exists, content is placed under an \"Introduction\" section.\n",
    "\n",
    "The scraped data is structured into a dictionary format for each country and returned. If the request fails, an error message is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles():\n",
    "    all_countries = ['India']\n",
    "    urls = [f\"https://www.whitecase.com/insight-our-thinking/ai-watch-global-regulatory-tracker-{each}\" for each in all_countries]\n",
    "\n",
    "    # Initialize an empty list to store all the scraped data\n",
    "    scraped_data = []\n",
    "\n",
    "    for country, url in zip(all_countries, urls):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            content = soup.find_all('div', class_='field field--name-body field--type-text-with-summary field--label-hidden field--item')\n",
    "            current_heading = None\n",
    "            country_data = {'Country': country, 'Sections': []}\n",
    "            for section in content:\n",
    "                paragraphs = section.find_all(['p', 'h2', 'h3', 'li'])\n",
    "                for paragraph in paragraphs:\n",
    "                    text = paragraph.get_text(strip=True)\n",
    "                    if paragraph.name in ['h2', 'h3']:\n",
    "                        current_heading = text\n",
    "                        country_data['Sections'].append({'Heading': current_heading, 'Content': []})\n",
    "                    else:\n",
    "                        if current_heading:\n",
    "                            country_data['Sections'][-1]['Content'].append(text)\n",
    "                        else:\n",
    "                            if not country_data['Sections']:\n",
    "                                country_data['Sections'].append({'Heading': 'Introduction', 'Content': []})\n",
    "                            country_data['Sections'][0]['Content'].append(text)\n",
    "            scraped_data.append(country_data)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the webpage for {country}. Status code: {response.status_code}\")\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ChromaDB\n",
    "\n",
    "- **`Access or Create Collection`:** \n",
    "     - The function will checks if a ChromaDB collection named \"articles1\" exists using client.get_collection(). If it doesn't, it creates a new one with client.create_collection().\n",
    "\n",
    "- **`Load Embedding Model`:**\n",
    "    - It loads the SentenceTransformer model 'all-MiniLM-L6-v2' to generate embeddings for the article text.\n",
    "\n",
    "- **`Process and Embed Articles`:**\n",
    "    - Now for each article in scraped_data, it concatenates headings and content into a single string, full_text. The model then converts this text into an embedding (a vector representation of the textwill get created).\n",
    "\n",
    "- **`Store in ChromaDB`:**\n",
    "    - Next this functions adds each article to ChromaDB with the document text, its embedding, metadata (country name), and a unique ID.\n",
    "\n",
    "- **`Return`:**\n",
    "    - This will prints how many articles were stored and returns the collection name \"articles1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chromadb(scraped_data):\n",
    "    # Try to get the existing collection or create a new one\n",
    "    collection_name = \"articles1\"\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get collection '{collection_name}': {e}\")\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "\n",
    "    # Load Sentence Transformer for embedding\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Add articles to ChromaDB\n",
    "    for i, article in enumerate(scraped_data):\n",
    "        full_text = \"\\n\".join([f\"{section['Heading']}\\n\" + \"\\n\".join(section['Content']) for section in article['Sections']])\n",
    "        \n",
    "        embedding = model.encode(full_text).tolist()\n",
    "\n",
    "        collection.add(\n",
    "            documents=[full_text],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{'title': article['Country']}],\n",
    "            ids=[str(i)]\n",
    "        )\n",
    "\n",
    "    print(f\"Stored {len(scraped_data)} articles in ChromaDB.\")\n",
    "    return collection_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the data\n",
    "\n",
    "- **`Process_articles function will pre-processes the scraped data to ensure the text is broken down into manageable chunks and embedded for efficient storage and retrieval in ChromaDB by performing the following steps`:**\n",
    "\n",
    "    - `1. Text Splitting`:\n",
    "        - It uses the RecursiveCharacterTextSplitter to break down large article text into smaller chunks. Each chunk has a size of 800 characters with a 200-character overlap for better context continuity.\n",
    "\n",
    "    - `2. Document Creation`: \n",
    "        - For each article, the text (combining headings and content) is transformed into a Document object, which is then split into chunks using the splitter.\n",
    "\n",
    "    - `3. Generate Unique IDs`:\n",
    "        - A unique ID is created for each chunk using the uuid4() function to identify them in the database.\n",
    "\n",
    "    - `4. Embedding and Storing`: \n",
    "        - The chunks are embedded using the OllamaEmbeddings model and stored in the Chroma vector database (local-rag collection) along with their unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(scraped_data):\n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "    \n",
    "    # Process and split the document text\n",
    "    chunks = []\n",
    "    for article in scraped_data:\n",
    "        document_text = \"\\n\".join([f\"{section['Heading']}\\n\" + \"\\n\".join(section['Content']) for section in article['Sections']])\n",
    "        document = Document(page_content=document_text)\n",
    "        chunks += text_splitter.split_documents([document])\n",
    "    \n",
    "    # Generate unique IDs for each chunk\n",
    "    chunk_ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "    # Initialize the vector database with the split chunks and the generated IDs\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "        ids=chunk_ids,\n",
    "        collection_name=\"local-rag\"\n",
    "    )\n",
    "\n",
    "    return vector_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model \n",
    "\n",
    "- **`Model used LLama 3`:**\n",
    "    - **`1. Model Initialization`:**\n",
    "\n",
    "        - The local language model llama3 is set as the model to use.An instance of ChatOllama, which is a large language model interface, is initialized with this model.\n",
    "\n",
    "    - **`2. Query Prompt Creation`:**\n",
    "\n",
    "        - A PromptTemplate named QUERY_PROMPT is created. It instructs the model to retrieve relevant information only from the vector database without using external knowledge, assumptions, or opinions. If the answer is not found in the retrieved documents, the model should explicitly state that the information is not available.\n",
    "\n",
    "    - **`3. Retriever Initialization`:**\n",
    "\n",
    "        - A MultiQueryRetriever is created using the language model and the vector database. This retriever handles the retrieval of relevant content from the vector database based on user questions. It uses the query prompt to reformulate the question and enhance the retrieval accuracy.\n",
    "\n",
    "    - **`4. Answer Prompt Template`:**\n",
    "\n",
    "        - A ChatPromptTemplate is created to structure the model's response. It provides a template that requires the model to answer the question based only on the context retrieved from the vector database.\n",
    "\n",
    "    - **`5. Chain Setup`:**\n",
    "\n",
    "        - A processing chain is created:\n",
    "            First, the retriever is used to retrieve context from the vector database.\n",
    "            The question is passed through using RunnablePassthrough() to maintain its structure.\n",
    "            The prompt template formats the input for the language model.\n",
    "            The language model (llm) generates a response based on the retrieved context.\n",
    "            The StrOutputParser processes the model’s output into a readable format.\n",
    "\n",
    "    - **`6. Return Chain`:**\n",
    "        - The function returns the complete chain that processes the question, retrieves relevant context from the vector database, and provides an answer based only on the retrieved content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chain(vector_db):\n",
    "    local_model = \"llama3\"\n",
    "    llm = ChatOllama(model=local_model)\n",
    "    \n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your sole task is to retrieve the most relevant information \n",
    "        strictly from a vector database containing stored articles and content. Your response must be based **only** on \n",
    "        the information retrieved from the provided documents. You are NOT allowed to use any external knowledge, personal \n",
    "        opinions, or make assumptions. Use the retrieved information strictly to answer the question in the most accurate \n",
    "        way possible. \n",
    "\n",
    "        If the relevant information is not present in the retrieved content, say explicitly: \"The answer is not found in \n",
    "        the documents provided.\"\n",
    "\n",
    "        Here are ten alternative ways to phrase the original question to improve retrieval performance and ensure the highest \n",
    "        likelihood of retrieving the correct answer.\n",
    "\n",
    "        Original question: {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_db.as_retriever(), \n",
    "        llm,\n",
    "        prompt=QUERY_PROMPT\n",
    "    )\n",
    "    \n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GraphRAG\n",
    "\n",
    "- **`What is GraphRAG`:**\n",
    "     - GraphRAG (Graph-based Retrieval-Augmented Generation) is a technique that combines knowledge graphs and neural language models to enhance the process of answering queries. \n",
    "     - It uses a graph structure to represent relationships between different pieces of data or documents, allowing for more structured and contextually rich information retrieval. \n",
    "     - Each node in the graph represents a document or piece of knowledge, while edges between nodes signify the relationships or connections between them. \n",
    "     - This structure is used to improve the generation of responses by combining both retrieval-based information and graph-based reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "- **`Initialize Graph and Data Lists`:**\n",
    "     - The script initializes lists for storing documents, metadatas, and ids for 15 text entries. It also initializes a directed graph (DiGraph) for GraphRAG.\n",
    "\n",
    "- **`Generate Text and IDs`:**\n",
    "    - A loop generates text1, text2, ..., text15 and corresponding id1, id2, ..., id15. Each document also has associated metadata (e.g., \"source\": \"text_info1\").\n",
    "\n",
    "- **`Add Nodes to Graph`:**\n",
    "     - Each generated text and metadata is added to the graph as nodes with document_id as the identifier.\n",
    "\n",
    "- **`Add Documents to ChromaDB`:**\n",
    "    - The generated documents, along with their metadata and IDs, are added to a ChromaDB collection named \"laws\".\n",
    "\n",
    "- **`Create Graph Connections`:**\n",
    "    - Consecutive nodes (id1 to id15) are linked with directed edges in the graph to show relationships between them (e.g., consecutive text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_graph_rag():\n",
    "    # Initialize lists for documents, metadatas, and ids\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    # Initialize GraphRAG\n",
    "    graph = nx.DiGraph()  # Use DiGraph for a directed graph, or Graph for an undirected graph\n",
    "\n",
    "    # Loop to generate text1, text2, ..., text15 and id1, id2, ..., id15\n",
    "    for i in range(1, 16):\n",
    "        text = f\"text{i}\"  # This generates text1, text2, ..., text15\n",
    "        document_id = f\"id{i}\"  # This generates id1, id2, ..., id15\n",
    "        metadata = {\"source\": f\"text_info{i}\"}  # This generates source metadata for each text\n",
    "\n",
    "        # Append to the respective lists\n",
    "        documents.append(text)\n",
    "        metadatas.append(metadata)\n",
    "        ids.append(document_id)\n",
    "\n",
    "        # Add nodes to the GraphRAG\n",
    "        graph.add_node(document_id, text=text, metadata=metadata)\n",
    "\n",
    "    # Add the generated data to ChromaDB collection\n",
    "    collection = client.get_or_create_collection(\"laws\")\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "    print(\"Documents added successfully to ChromaDB!\")\n",
    "\n",
    "    # Link nodes as edges (for simplicity, connecting consecutive nodes)\n",
    "    for i in range(1, 15):\n",
    "        graph.add_edge(f\"id{i}\", f\"id{i+1}\", relation=\"consecutive\")\n",
    "\n",
    "    print(\"Nodes and edges added to GraphRAG!\")\n",
    "    return graph, collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation\n",
    "\n",
    "- **`Query ChromaDB`:**\n",
    "    - It queries the ChromaDB collection for documents relevant to the provided question. The query retrieves the top 5 documents based on their relevance.\n",
    "\n",
    "- **`Display Top Document`:**\n",
    "     - The function extracts and prints the top document from the query results. This document is considered the most relevant to the question.\n",
    "\n",
    "- **`Fetch Connected Nodes from GraphRAG`:**\n",
    "    - It retrieves the document ID from the query results. If the ID is a list, it selects the first element.\n",
    "    - It checks if the document ID exists as a node in the GraphRAG. If it does, the function retrieves and prints the connected nodes (successors) from the graph, indicating relationships or related documents.\n",
    "    - If no connected nodes are found or the document ID is not in the graph, it prints an appropriate message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb_and_graph(question, collection, graph):\n",
    "    # Query ChromaDB for top document\n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    top_document = results['documents'][0]\n",
    "    \n",
    "    # Display top document\n",
    "    print(f\"Top document found: {top_document}\")\n",
    "    \n",
    "    # Fetch connected nodes from GraphRAG\n",
    "    doc_id = results['ids'][0]  # Get the id of the top document (extracting from the list)\n",
    "    if isinstance(doc_id, list):  # In case doc_id is a list, get the first element\n",
    "        doc_id = doc_id[0]\n",
    "    \n",
    "    if doc_id in graph.nodes:\n",
    "        connected_nodes = list(graph.successors(doc_id))  # Get connected nodes\n",
    "        if connected_nodes:\n",
    "            print(f\"Related nodes from GraphRAG: {connected_nodes}\")\n",
    "        else:\n",
    "            print(\"No related nodes found.\")\n",
    "    else:\n",
    "        print(\"Document ID not found in GraphRAG.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Governance Tracker Chatbot\n",
      "Scraping articles...\n",
      "Scraping complete. Adding to ChromaDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsrak\\Desktop\\case_study\\AI_governance\\mynewenv1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 1 articles in ChromaDB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 14/14 [00:32<00:00,  2.35s/it]\n",
      "Insert of existing embedding ID: id1\n",
      "Insert of existing embedding ID: id2\n",
      "Insert of existing embedding ID: id3\n",
      "Insert of existing embedding ID: id4\n",
      "Insert of existing embedding ID: id5\n",
      "Insert of existing embedding ID: id6\n",
      "Insert of existing embedding ID: id7\n",
      "Insert of existing embedding ID: id8\n",
      "Insert of existing embedding ID: id9\n",
      "Insert of existing embedding ID: id10\n",
      "Insert of existing embedding ID: id11\n",
      "Insert of existing embedding ID: id12\n",
      "Insert of existing embedding ID: id13\n",
      "Insert of existing embedding ID: id14\n",
      "Insert of existing embedding ID: id15\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n",
      "Add of existing embedding ID: id3\n",
      "Add of existing embedding ID: id4\n",
      "Add of existing embedding ID: id5\n",
      "Add of existing embedding ID: id6\n",
      "Add of existing embedding ID: id7\n",
      "Add of existing embedding ID: id8\n",
      "Add of existing embedding ID: id9\n",
      "Add of existing embedding ID: id10\n",
      "Add of existing embedding ID: id11\n",
      "Add of existing embedding ID: id12\n",
      "Add of existing embedding ID: id13\n",
      "Add of existing embedding ID: id14\n",
      "Add of existing embedding ID: id15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added successfully to ChromaDB!\n",
      "Nodes and edges added to GraphRAG!\n",
      "Top document found: ['text8', 'text14', 'text5', 'text2', 'text3']\n",
      "Related nodes from GraphRAG: ['id9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided context, it can be answered as follows:\n",
      "\n",
      "Currently, there are no specific codified laws, statutory rules or regulations in India that directly regulate AI.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"AI Governance Tracker Chatbot\")\n",
    "    \n",
    "    # Scrape and process articles\n",
    "    print(\"Scraping articles...\")\n",
    "    scraped_data = scrape_articles()\n",
    "    print(\"Scraping complete. Adding to ChromaDB...\")\n",
    "    \n",
    "    collection_name = add_to_chromadb(scraped_data)\n",
    "    \n",
    "    # Process articles into chunks for ChromaDB\n",
    "    vector_db = process_articles(scraped_data)\n",
    "    \n",
    "    # Initialize the LLM retrieval chain\n",
    "    chain = initialize_chain(vector_db)\n",
    "    \n",
    "    # Initialize GraphRAG\n",
    "    graph, graph_collection = initialize_graph_rag()\n",
    "    \n",
    "    question = input(\"Enter your question: \")\n",
    "    \n",
    "    if question:\n",
    "        # Query ChromaDB and GraphRAG for related content\n",
    "        query_chromadb_and_graph(question, graph_collection, graph)\n",
    "        response = chain.invoke(question)\n",
    "        print(\"Response:\", response)\n",
    "    else:\n",
    "        print(\"Please enter a question.\")\n",
    "    \n",
    "    cleanup = input(\"Would you like to cleanup the ChromaDB? (yes/no): \")\n",
    "    if cleanup.lower() == 'yes':\n",
    "        client.delete_collection(name=collection_name)\n",
    "        print(\"ChromaDB cleaned up.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama3 LLM Modal card\n",
    "\n",
    "- **`Model Overview`:**\n",
    "    - Model Name: ChatOllama\n",
    "    - Model Type: Large Language Model (LLM)\n",
    "    - Usage: The ChatOllama model is used to generate responses by retrieving and processing information from articles stored in a vector database (ChromaDB) and a graph structure (GraphRAG).\n",
    "\n",
    "### Intended Use\n",
    "\n",
    "- **`Primary Use Case`:**\n",
    "    To answer user queries related to AI governance by referencing a curated set of articles and documents.\n",
    "    To act as an informational tool, offering insights into AI regulations and policies based on pre-scraped data.\n",
    "\n",
    "- **`Intended Users`**:\n",
    "    Researchers and policy makers interested in AI governance and regulatory developments.\n",
    "    Organizations and individuals looking to stay informed about AI regulations and best practices.\n",
    "    Anyone needing a reliable source for AI governance information that is based on a specific set of documents.\n",
    "\n",
    "### Performance\n",
    "\n",
    "- **`Accuracy`**:\n",
    "    The model's accuracy is tied to its ability to extract and generate responses based on the documents stored in ChromaDB.\n",
    "    Its responses depend heavily on the relevance and completeness of the scraped articles, as it doesn't have access to information beyond what is stored in the database.\n",
    "\n",
    "### Limitations\n",
    "- **`Model Limitations`:**\n",
    "    - Contextual Understanding: The model may struggle with understanding nuanced or context-specific questions if the required information is not available in the retrieved documents.\n",
    "    - Training Data Dependency: Since the model’s responses are solely based on the retrieved articles, its performance is directly tied to the quality and scope of the data in ChromaDB.\n",
    "- **`Performance Variability`:**\n",
    "    The model's accuracy may vary based on the comprehensiveness of the documents in ChromaDB. If the documents are biased or incomplete, this can affect the model's responses.\n",
    "    Limited to the scope of the pre-scraped articles; it cannot provide information outside of this dataset.\n",
    "\n",
    "\n",
    "### Biases\n",
    "- **`Document Bias`:**\n",
    "    - If the query asked is biased, the model's answers may reflect that bias.\n",
    "\n",
    "- **`Query Bias`:**\n",
    "    - The phrasing of questions can influence the model’s ability to retrieve relevant information, potentially leading to biased or incomplete responses.\n",
    "\n",
    "\n",
    "### Security & Privacy\n",
    "- **`Data Security`:**\n",
    "    - Ensure that the scraped documents do not contain sensitive or personal information. User queries should be handled securely, with no storage of personal data.\n",
    "    \n",
    "- **`Model Attacks`:**\n",
    "The system could be exposed to adversarial attacks through crafted queries designed to exploit the retrieval or response generation process. Measures should be in place to identify and mitigate such risks.\n",
    "\n",
    "### Ethical Considerations\n",
    "- **`Ethical Use`:**\n",
    "    - The system should be used ethically, respecting user privacy and adhering to ethical standards in AI. This includes providing accurate, unbiased information and not using the system to disseminate misinformation.\n",
    "    - Users should be informed about the limitations and scope of the system to set correct expectations.\n",
    "\n",
    "- **`Future Work`:**\n",
    "    - 1. Enhancing Explainability: Further work is needed to make the model’s decision-making process more transparent and understandable to end-users.\n",
    "     - 2. Addressing Biases: Implementing methods to identify and mitigate biases in the source documents and responses, ensuring a more balanced and fair output.\n",
    "    - 3. Expanding Document Coverage: Continually update the document set in ChromaDB to include the latest developments in AI governance, ensuring the model stays relevant and comprehensive.\n",
    "    - 4. Exploring Additional XAI Methods: Investigate additional explainable AI methods to better understand the model's processing and improve user trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Assessment\n",
    "\n",
    "- **`Data Bias`:**\n",
    "\n",
    "    - **Issue**: Embedding models can inherit and amplify biases present in their training data, potentially leading to skewed or unfair outcomes in the responses.\n",
    "    - **Impact on Model**: If the articles and documents in ChromaDB contain biases, the model may provide biased information, reflecting a narrow viewpoint.It depends on the prompt we give and how accurate the prompt is asked.\n",
    "\n",
    "- **`Adversarial Attacks`:**\n",
    "    - **Issue**: The model might be vulnerable to adversarial attacks, where slightly altered input can lead to significantly different outputs, undermining the model's reliability.\n",
    "     - **Impact on Model**: Users could craft queries to exploit the model's weaknesses, causing it to retrieve irrelevant or misleading information.\n",
    "\n",
    "- **`Privacy Risks`:**\n",
    "    - **Issue**: If the model processes sensitive data, embeddings might inadvertently reveal personal information.\n",
    "    - **Impact on Model**: There's a risk of exposing sensitive information if the scraped content includes identifiable data or if user queries are not handled securely.\n",
    "\n",
    "- **`Overcoming Challenges`:**\n",
    "  - **Bias Mitigation**: Employ bias detection and mitigation strategies during model training and before deployment. Regularly update the model with more diverse and representative training data.\n",
    "   - **Robustness Testing**: Implement adversarial testing and robustness checks to identify and mitigate vulnerabilities to attacks, ensuring consistent and reliable outputs.\n",
    "   - **Privacy Preservation**: Use techniques such as differential privacy during training to minimize the risk of exposing sensitive information through the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
